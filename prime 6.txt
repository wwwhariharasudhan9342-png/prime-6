import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto"
)

tokenizer.use_default_system_prompt = False

conversation = []

print("ðŸ¤– Chatbot initialized!")
print("Type 'exit' to stop chatting.\n")

while True:
    user_input = input("User: ")

    if user_input.lower() == "exit":
        print("Chat ended.")
        break

    conversation.append({"role": "user", "content": user_input})

    inputs = tokenizer.apply_chat_template(
        conversation,
        return_tensors="pt",
        add_generation_prompt=True
    )

    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    outputs = model.generate(
        **inputs,
        max_new_tokens=150,
        do_sample=True,
        temperature=0.7,
        top_p=0.9
    )

    input_len = inputs["input_ids"].shape[1]
    response = tokenizer.decode(
        outputs[0][input_len:],
        skip_special_tokens=True
    )

    print("Bot:", response)

    conversation.append({"role": "assistant", "content": response})